{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u5i1cHX_3tb"
   },
   "source": [
    "# Datathon Interbank Internacional 2019\n",
    "Panuccio Abraham Alan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dX-uo7ekLjDw"
   },
   "source": [
    "# Preparacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-V2iEcHABdr"
   },
   "source": [
    "## Librerias y configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11776,
     "status": "error",
     "timestamp": 1574042160464,
     "user": {
      "displayName": "Tz Alan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDS8XCsXwaMPcm9Ni1yodQ9orQCuEqevC_Mpp5m=s64",
      "userId": "12064921321119825687"
     },
     "user_tz": 180
    },
    "id": "0NfUfEzf7yqX",
    "outputId": "68c80493-5b04-44e0-e114-18d10abe8c36"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import string\n",
    "import functools\n",
    "from importlib import reload\n",
    "import functools\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from sklearn.preprocessing import KBinsDiscretizer  \n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.optimize import differential_evolution\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import custom_feature_selection as cfs\n",
    "#import custom_constants as cc\n",
    "\n",
    "BASE_PATH=\"/content/gdrive/My Drive/Competencias DM/interbank-internacional-2019/\"\n",
    "\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "cff = SourceFileLoader(\"custom_features_functions\", BASE_PATH+\"custom_features_functions.py\").load_module()\n",
    "cc = SourceFileLoader(\"custom_constants\", BASE_PATH+\"custom_constants.py\").load_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xl7SsMk7yqd"
   },
   "source": [
    "## Lectura de Datos\n",
    "\n",
    "Se leerán las bases básicas, que solo tienen registros a nivel de usuario. Sólo se leerá la información de campaña que dependa del tiempo. Queda para mejorar, la incorporacion de más informaición temporal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "5YstXL7B7yqe"
   },
   "outputs": [],
   "source": [
    "# Bases de training y testing\n",
    "train = pd.read_csv(BASE_PATH+\"ib_base_inicial_train.csv\")\n",
    "X_test = pd.read_csv(BASE_PATH+\"ib_base_inicial_test.csv\")\n",
    "\n",
    "# Datos agregados\n",
    "sunat = pd.read_csv(BASE_PATH+\"ib_base_sunat.csv\")\n",
    "reniec = pd.read_csv(BASE_PATH+\"ib_base_reniec.csv\")\n",
    "vehicular = pd.read_csv(BASE_PATH+\"ib_base_vehicular.csv\")\n",
    "digital = pd.read_csv(BASE_PATH+\"ib_base_digital.csv\")\n",
    "\n",
    "# Las basese mas grandes van en zip\n",
    "rcc = pd.read_csv(BASE_PATH+\"ib_base_rcc.zip\")\n",
    "campanias = pd.read_csv(BASE_PATH+\"ib_base_campanias.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wABXQHO4AImw"
   },
   "outputs": [],
   "source": [
    "X_test[\"codtarget\"] = 0\n",
    "X_test[\"margen\"] = 0\n",
    "train[\"codtarget\"] = (train[\"margen\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcWft-zJ_zH8"
   },
   "source": [
    "## Fuentes de Train y Test\n",
    "Comienzo analizando las fuentes de datos de train y testing, para entender de que se tratan y que variables puedo agregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4lErjsz--U1"
   },
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66Qd5BMGBhXN"
   },
   "outputs": [],
   "source": [
    "train.groupby([\"codmes\"])[[\"id_persona\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_F8k3MWEU0H"
   },
   "source": [
    "### Correlacion de variables vs target\n",
    "Si analizamos la correlacion de las variables de dt vs el target, vemos que el id_persona es la variable más correlacionada positivamente con el target, esto puede deberse a que sea un autoincremental, entonces lo que no estaría diciendo es que son clientes más \"antiguos\", la variables que entiendo esta influyendo no es el id sino la antiguedad del cliente como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jg4DXFlzAQl7"
   },
   "outputs": [],
   "source": [
    "train.corr()[[\"codtarget\"]].sort_values(by=\"codtarget\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1MvAzqXD84A"
   },
   "outputs": [],
   "source": [
    "# Analizamos la distribución de la probabilidad del evento en N bins \n",
    "# creados en base al id_persona y vemos como se diferencian los id más altos de los más bajos.\n",
    "bins = []\n",
    "for i in range(20):\n",
    "  train2 = copy.copy(train)\n",
    "  cff.scale_feature(train2, [\"id_persona\"], i, preprocessing.MaxAbsScaler())\n",
    "  rr = train2.groupby([\"SCL_id_persona\"])[[\"codtarget\"]].mean().sort_values([\"codtarget\"], ascending=False).head(1)\n",
    "  bins.append({\"bins\":str(i), \"P\":round(list(rr[\"codtarget\"])[0], 4)})\n",
    "\n",
    "# Elijo la cantidad de bins que deja el corte con mayor probabilidad de convertir\n",
    "bins = pd.DataFrame(bins)\n",
    "bins = bins.sort_values(by=[\"P\"], ascending=False).head(1)\n",
    "\n",
    "# Realizo el bining tanto en train como en test\n",
    "cff.scale_feature(train, [\"id_persona\"], int(bins[\"bins\"].iloc[0]), preprocessing.MaxAbsScaler())\n",
    "cff.scale_feature(X_test, [\"id_persona\"], int(bins[\"bins\"].iloc[0]), preprocessing.MaxAbsScaler())\n",
    "\n",
    "# Veo si la correlación mejora bajando mucho la dimensionalidad de la variable y minimizando el overfitting\n",
    "train.corr()[[\"codtarget\"]].sort_values(by=\"codtarget\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QPHRgRfJJpg"
   },
   "outputs": [],
   "source": [
    "# Ahora analizo las variables financieras con mayor correlacion\n",
    "# intuyendo que un cociente entre alguna de estas pueda explicar mejor la varianza\n",
    "for dt in [train, X_test]:\n",
    "  dt[\"c_linea_ingreso\"] = list(map(lambda x,y: 0 if y==0 and x==0 else 100 if y==0 and x >0 else round(10*x/y), dt[\"linea_ofrecida\"], dt[\"ingreso_neto\"]))\n",
    "  dt[\"c_ingreso_cem\"] = list(map(lambda x,y: 0 if y==0 and x==0 else 100 if y==0 and x >0 else round(10*x/y), dt[\"ingreso_neto\"], dt[\"cem\"]))\n",
    "\n",
    "train.corr()[[\"codtarget\"]].sort_values(by=\"codtarget\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SMoJAuNVrEu"
   },
   "source": [
    "## Base SUNAT - Actividades Económicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNE0Z-85UZw6"
   },
   "outputs": [],
   "source": [
    "# Por lo pronto no voy a usar esta fuente, porque no especifíca a que mes es la foto y hay clientes con mas de un registro, con lo cual \n",
    "# podría ser un falso predictor\n",
    "sunat.groupby([\"id_persona\"])[[\"activ_econo\"]].count().sort_values(by=\"activ_econo\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhT86pHpXTm-"
   },
   "source": [
    "## Base Reniec - Variables Sociodemográficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljM1oSk-_LuU"
   },
   "outputs": [],
   "source": [
    "reniec.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpatbsVTXhij"
   },
   "source": [
    "### Correlacion de variables vs target\n",
    "Si analizamos la correlacion de las variables de dt vs el target, vemos que variable soc_var1 es la variable más correlacionada positivamente con el target, no tenemos muchos detalles sobre que es cada variable como para sacar conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQ1-93I4VL0p"
   },
   "outputs": [],
   "source": [
    "reneic_tg = train.merge(reniec, how=\"left\", on=[\"id_persona\"])\n",
    "dt_corr = reneic_tg.corr()[[\"codtarget\"]]\n",
    "dt_corr[\"codtarget\"] = abs(dt_corr[\"codtarget\"])\n",
    "dt_corr.sort_values(by=\"codtarget\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChR1gn4b6H6a"
   },
   "outputs": [],
   "source": [
    "dt_corr = train.corr()[[\"codtarget\"]]\n",
    "dt_corr[\"codtarget\"] = abs(dt_corr[\"codtarget\"])\n",
    "dt_corr.sort_values(by=\"codtarget\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnMFEBfIYHau"
   },
   "outputs": [],
   "source": [
    "del reneic_tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oADh7BdBYJZs"
   },
   "source": [
    "## Base Vehicular - Datos sobre el vehículo que tiene y variables relacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fj2lDP9n_NON"
   },
   "outputs": [],
   "source": [
    "vehicular.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74JVxb5FYSu5"
   },
   "source": [
    "### Desnormalizacion de datos y tabla resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bx3kp4DUYZcR"
   },
   "outputs": [],
   "source": [
    "# Para el dataset final, necesito una tabla con un dato por persona\n",
    "\n",
    "# Para esto primero armo datasets con el valor total por persona de cada columna (media, promedio, max, min)\n",
    "var1_ag = vehicular.groupby([\"id_persona\"], as_index=False)[\"veh_var1\"].agg({'sum_var1' : np.sum, 'avg_var1' : np.mean, 'min_var1' : np.min, 'max_var1' : np.max})\n",
    "var2_ag = vehicular.groupby([\"id_persona\"], as_index=False)[\"veh_var2\"].agg({'sum_var2' : np.sum, 'avg_var2' : np.mean, 'min_var2' : np.min, 'max_var2' : np.max})\n",
    "var1_ag.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hlD7Fc8cw3O"
   },
   "outputs": [],
   "source": [
    "vehicular[[\"marca\"]].drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56Z7kdJOsBKv"
   },
   "source": [
    "### Cateogoría propia de automóviles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQEtLos7cR2P"
   },
   "outputs": [],
   "source": [
    "# Para realizar una apertura por marca de automovil, agrupo los automoviles en 25 categoricas dependiendo del valor de las\n",
    "# variables 1 y 2 y luego calculo variables separadas para dichas variables\n",
    "marcas_var1_ag = vehicular.groupby([\"marca\"], as_index=False)[\"veh_var1\"].median().fillna(0)\n",
    "marcas_var2_ag = vehicular.groupby([\"marca\"], as_index=False)[\"veh_var2\"].median().fillna(0)\n",
    "#marcas_var1_ag.head(5)\n",
    "\n",
    "# Realizo el bining tanto en train como en test\n",
    "cff.binning_feature(marcas_var1_ag, [\"veh_var1\"], 1, KBinsDiscretizer(n_bins=25, encode='ordinal', strategy='uniform'))\n",
    "cff.binning_feature(marcas_var2_ag, [\"veh_var2\"], 1, KBinsDiscretizer(n_bins=25, encode='ordinal', strategy='uniform'))\n",
    "\n",
    "# Cambio el nombre de las columnas\n",
    "marcas_var1_ag.columns = [\"marca\",\"veh_var1\",\"cat_var1\"]\n",
    "marcas_var2_ag.columns = [\"marca\",\"veh_var2\",\"cat_var2\"]\n",
    "marcas_var1_ag[\"cat_var1\"] = marcas_var1_ag[\"cat_var1\"].astype(\"str\")\n",
    "marcas_var2_ag[\"cat_var2\"] = marcas_var2_ag[\"cat_var2\"].astype(\"str\")\n",
    "\n",
    "# Agrego las nuevas categorias al dataset original\n",
    "vehicular_cat = vehicular.merge(marcas_var1_ag[[\"marca\",\"cat_var1\"]], how=\"left\", on=\"marca\")\n",
    "vehicular_cat = vehicular_cat.merge(marcas_var2_ag[[\"marca\",\"cat_var2\"]], how=\"left\", on=\"marca\")\n",
    "vehicular_cat.head(10)\n",
    "\n",
    "dt_vehiculos = pd.DataFrame({\"id_persona\":vehicular_cat[\"id_persona\"].drop_duplicates()})\n",
    "for i in [1,2]:\n",
    "  vehicular_end = vehicular_cat.groupby([\"id_persona\", \"cat_var\"+str(i)])[\"veh_var\"+str(i)].sum().unstack(level=1, fill_value=0).astype(\"float32\")\n",
    "  vehicular_end.columns = [\"sum_v\"+str(i)+\"_cat\"+c for c in vehicular_end.columns]\n",
    "  dt_vehiculos = dt_vehiculos.merge(vehicular_end, how=\"left\", on=\"id_persona\")\n",
    "  del vehicular_end\n",
    "  \n",
    "  vehicular_end = vehicular_cat.groupby([\"id_persona\", \"cat_var\"+str(i)])[\"veh_var\"+str(i)].mean().unstack(level=1, fill_value=0).astype(\"float32\")\n",
    "  vehicular_end.columns = [\"avg_v\"+str(i)+\"_cat\"+c for c in vehicular_end.columns]\n",
    "  dt_vehiculos = dt_vehiculos.merge(vehicular_end, how=\"left\", on=\"id_persona\")\n",
    "  del vehicular_end\n",
    "\n",
    "print(dt_vehiculos.shape)\n",
    "dt_vehiculos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gh0OwJG-r3Aq"
   },
   "source": [
    "### Correlacion de variables vs target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNPWEZpSp799"
   },
   "outputs": [],
   "source": [
    "# Ahora analizo la correlación de dichas variables y puedo ver que las categorias más altas (de valores más altos en las variables)\n",
    "# son las que mayor correlación tienen con el target\n",
    "dt_vehiculos_tg = train[[\"codmes\",\"id_persona\",\"codtarget\"]].merge(dt_vehiculos, on=[\"id_persona\"])\n",
    "dt_cors = dt_vehiculos_tg.corr()[[\"codtarget\"]]\n",
    "dt_cors = abs(dt_cors)\n",
    "dt_cors.drop([\"codtarget\",\"id_persona\"], axis=0).sort_values(by=\"codtarget\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zs1MZ2icxbCO"
   },
   "outputs": [],
   "source": [
    "# Scoring features focused in the conversion target\n",
    "print(\"     Feature Selection [ \", end = '')\n",
    "final_iv, scored_fe = cff.score_features(dt_vehiculos_tg.drop([\"codmes\"],axis=1), dt_vehiculos_tg.codtarget)\n",
    "scored_fe   = scored_fe.rename(columns={'VAR_NAME':'index'})\n",
    "selected_fe = scored_fe.sort_values(['IV'],ascending=0).head(10)\n",
    "selected_fe = selected_fe.set_index('index').index\n",
    "print(\" ]\")\n",
    "del dt_vehiculos_tg\n",
    "\n",
    "# Always we need the index columns to do the join after\n",
    "col_index = pd.Index([\"id_persona\"]) \n",
    "col_index = col_index.append(selected_fe)\n",
    "col_index = list(set(col_index))\n",
    "col_index\n",
    "\n",
    "# At the end we keep just the selected columns and the index columns\n",
    "dt_vehiculos = dt_vehiculos[col_index]\n",
    "dt_vehiculos.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UehTMHJmsGcf"
   },
   "source": [
    "## Base Digital\n",
    "Esta fuente es diaria, para empezar unificaremos la fuente en una mensual y después en base asu correlación con la variable target, vamos a realizar un análisis por ventanas de tiempo y de tendencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4bwR5p__Ocs"
   },
   "outputs": [],
   "source": [
    "print(digital.shape)\n",
    "digital.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzZe9eoK8C3X"
   },
   "outputs": [],
   "source": [
    "# Empiezo creando un agrupado por mes, con la suma total de cada mes \n",
    "# y a demas la cantidad de dias distintos\n",
    "digital[\"month\"] = list(map(lambda datee: datetime(int(str(datee)[:4]), int(str(datee)[4:6]), 1), digital[\"codday\"]))\n",
    "digital_mes = digital.groupby([\"id_persona\",\"month\"], as_index=False).sum()\n",
    "digital_mes.drop([\"codday\"], axis=1, inplace=True)\n",
    "digital_mes[\"codday\"] = digital.groupby([\"id_persona\",\"month\"], as_index=False)[\"codday\"].count()[\"codday\"]\n",
    "\n",
    "#Paso a int el mes\n",
    "digital_mes[\"month_int\"] = digital_mes['month'].dt.strftime(\"%Y%m\").astype(int)\n",
    "\n",
    "print(digital_mes.shape)\n",
    "digital_mes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eB_I2MXnD-hj"
   },
   "outputs": [],
   "source": [
    "digital_mes.groupby([\"month_int\"])[\"id_persona\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Je7pdaZZ9iA6"
   },
   "source": [
    "### Variables historicas por ventanas de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nesZsG-x-iY7"
   },
   "outputs": [],
   "source": [
    "columns_to_ignore = [\"month\", \"month_int\", \"id_persona\"]\n",
    "data_temp = copy.copy(digital_mes)\n",
    "data_temp.sort_values(by=[\"id_persona\",\"month\"], ascending=False)\n",
    "\n",
    "# Initializing the result dataset\n",
    "digital_hist = []\n",
    "q_columns = len(data_temp.drop(columns_to_ignore, axis=1).columns)\n",
    "q_col = 0\n",
    "global q_max2\n",
    "\n",
    "for c in data_temp.drop(columns_to_ignore, axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "\n",
    "    #traigo a la linea del mes los datos del mes anterior, -2 y -3\n",
    "    data_temp[c + \"_M1\"] = data_temp.groupby(['id_persona'], as_index=False)[c].shift(1)\n",
    "    data_temp[c + \"_M2\"] = data_temp.groupby(['id_persona'], as_index=False)[c].shift(2)\n",
    "    #data_temp[c + \"_M3\"] = data_temp.groupby(['id_persona'], as_index=False)[c].shift(3)\n",
    "\n",
    "    #Calculo la suma mes actual + 3\n",
    "    data_temp[\"sm_\"+c]= list(map(lambda t2, t1, t: t+t1+t2, data_temp[c + \"_M2\"], data_temp[c + \"_M1\"], data_temp[c]))\n",
    "\n",
    "    #Calculo el promedio mes actual + 3\n",
    "    data_temp[\"avg_\"+c]= list(map(lambda t2, t1, t: cff.promedio([t, t1, t2]), data_temp[c + \"_M2\"], data_temp[c + \"_M1\"], data_temp[c]))\n",
    "\n",
    "    # Normalizo las variables antes de calcular la tendencia\n",
    "    cff.scale_feature(data_temp, [c + \"_M2\", c + \"_M1\", c], 100, preprocessing.MaxAbsScaler())\n",
    "    \n",
    "     #Calculo la tendencia actual+1 vs +2+3\n",
    "    data_temp[\"td_\"+c]= list(map(lambda t2, t1, t: cff.tendencia((t+t1)/2, (t1+t2)/2), data_temp[\"SCL_\"+c + \"_M2\"], data_temp[\"SCL_\"+c + \"_M1\"], data_temp[\"SCL_\"+c]))\n",
    "\n",
    "    # Elimino las variables auxiliares creadas\n",
    "    data_temp = data_temp.drop(c + \"_M1\",axis=1)\n",
    "    data_temp = data_temp.drop(c + \"_M2\",axis=1)\n",
    "    #data_temp = data_temp.drop(c + \"_M3\",axis=1)\n",
    "    delete=pd.DataFrame(data_temp.filter(like='SCL_').columns).set_index(0).index\n",
    "    data_temp=data_temp.drop(delete,axis=1)\n",
    "\n",
    "    digital_hist.append(data_temp)\n",
    "\n",
    "digital_hist = pd.concat(digital_hist, axis=1)\n",
    "digital_hist = digital_hist.loc[:,~digital_hist.columns.duplicated()]\n",
    "\n",
    "delete=pd.DataFrame(digital_hist.filter(like='M3').columns).set_index(0).index\n",
    "digital_hist = digital_hist.drop(delete, axis=1)\n",
    "\n",
    "delete=pd.DataFrame(digital_hist.filter(like='M2').columns).set_index(0).index\n",
    "digital_hist = digital_hist.drop(delete, axis=1)\n",
    "\n",
    "delete=pd.DataFrame(digital_hist.filter(like='M1').columns).set_index(0).index\n",
    "digital_hist = digital_hist.drop(delete, axis=1)\n",
    "\n",
    "delete=pd.DataFrame(digital_hist.filter(like='SCL_').columns).set_index(0).index\n",
    "digital_hist = digital_hist.drop(delete, axis=1)\n",
    "\n",
    "digital_hist.fillna(0)\n",
    "digital_hist.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eSqF3OAGaUM"
   },
   "outputs": [],
   "source": [
    "# Ahora analizo la correlación de dichas variables y veo que variables resultaron más correlacionadas\n",
    "X_train2 = copy.copy(train)\n",
    "X_train2[\"month\"] = list(map(lambda datee: datetime(int(str(datee)[:4]), int(str(datee)[4:6]), 1), X_train2[\"codmes\"]))\n",
    "\n",
    "dt_result_tg = X_train2[[\"month\",\"id_persona\",\"codtarget\"]].merge(digital_hist, on=[\"id_persona\", \"month\"])\n",
    "dt_cors = dt_result_tg.corr()[[\"codtarget\"]]\n",
    "dt_cors = abs(dt_cors)\n",
    "dt_cors.drop([\"codtarget\",\"id_persona\",\"month_int\"], axis=0).sort_values(by=\"codtarget\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TPEKlYczSfu"
   },
   "outputs": [],
   "source": [
    "# Scoring features focused in the conversion target\n",
    "print(\"     Feature Selection [ \", end = '')\n",
    "final_iv, scored_fe = cff.score_features(dt_result_tg, dt_result_tg.codtarget)\n",
    "scored_fe   = scored_fe.rename(columns={'VAR_NAME':'index'})\n",
    "selected_fe = scored_fe.sort_values(['IV'],ascending=0).head(30)\n",
    "selected_fe = selected_fe.set_index('index').index\n",
    "print(\" ]\")\n",
    "\n",
    "# Always we need the index columns to do the join after\n",
    "col_index = pd.Index([\"month\",\"id_persona\"]) \n",
    "col_index = col_index.append(selected_fe)\n",
    "col_index = list(set(col_index))\n",
    "col_index\n",
    "\n",
    "# At the end we keep just the selected columns and the index columns\n",
    "digital_hist = digital_hist[col_index]\n",
    "del dt_result_tg\n",
    "digital_hist.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEgzJdQLmOwk"
   },
   "source": [
    "## Base RCC - Deuda otros bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVxLO75l_GnL"
   },
   "outputs": [],
   "source": [
    "rcc = pd.read_csv(BASE_PATH+\"ib_base_rcc.zip\")\n",
    "#rcc.groupby([\"codmes\"])[[\"id_persona\"]].count()\n",
    "rcc.groupby([\"producto\"])[[\"id_persona\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v8RimuyJoYC"
   },
   "outputs": [],
   "source": [
    "cod_bancos = rcc.groupby([\"cod_banco\"], as_index=False)[[\"mto_saldo\"]].median()\n",
    "cod_bancos = cod_bancos.sort_values(by=[\"mto_saldo\"], ascending=False)\n",
    "cff.binning_feature(cod_bancos, [\"mto_saldo\"], 1, KBinsDiscretizer(n_bins=200, encode='ordinal', strategy='uniform'))\n",
    "cod_bancos[\"BN_mto_saldo\"] = list(map(lambda b: 0 if b==0 else 1 if b==1 else 2 if b ==2 else 3, cod_bancos[\"BN_mto_saldo\"]))\n",
    "rcc = rcc.merge(cod_bancos[[\"cod_banco\",\"BN_mto_saldo\"]], on=[\"cod_banco\"], how=\"left\")\n",
    "#rcc.head(5)\n",
    "rcc.groupby([\"BN_mto_saldo\"])[[\"id_persona\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMqpmPYPNMr2"
   },
   "outputs": [],
   "source": [
    "keys = dict()\n",
    "keys[\"AVANCE\"]=\"OTROS\"\n",
    "keys[\"CARTAS_DE_CREDITO_OTORGADAS\"]=\"OTROS\"\n",
    "keys[\"CARTAS_FIANZAS_OTORGADAS\"]=\"OTROS\"\n",
    "keys[\"CREDITOS_CASTIGOS\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"CREDITO_CASTIGADOS_SIENDO_AMORTIZADOS\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"CREDITO_INMOBILIARIOS\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"DERIVADOS_ME_--_FORWARDS\"]=\"OTROS\"\n",
    "keys[\"DERIVADOS_ME_--_OPCIONES\"]=\"OTROS\"\n",
    "keys[\"DESCUENTOS\"]=\"OTROS\"\n",
    "keys[\"FACTORING\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"FINANC_COMEX\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"HIPOTECARIO_MIVIVIENDA\"]=\"PRESTAMOS\"\n",
    "keys[\"HIPOTECARIO_REGULAR\"]=\"PRESTAMOS\"\n",
    "keys[\"LEASING\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"LINEA_TOTAL_TC\"]=\"TARJETAS\"\n",
    "keys[\"OTROS_CREDITOS\"]=\"PRESTAMOS\"\n",
    "keys[\"PRESTAMOS_COMERCIALES\"]=\"PRESTAMOS_COMERCIALES\"\n",
    "keys[\"PRESTAMO_PERSONAL\"]=\"PRESTAMOS\"\n",
    "keys[\"REFINANCIADOS\"]=\"PRESTAMOS\"\n",
    "keys[\"RESTO_RD\"]=\"OTROS\"\n",
    "keys[\"RESTO_RI\"]=\"OTROS\"\n",
    "keys[\"SOBREGIRO\"]=\"OTROS\"\n",
    "keys[\"TARJETAS_COMPRAS\"]=\"TARJETAS\"\n",
    "keys[\"TARJETAS_EFECTIVO\"]=\"TARJETAS\"\n",
    "keys[\"TARJETAS_OTROS_CONCEPTOS\"]=\"TARJETAS\"\n",
    "keys[\"TARJETAS_SIN_DEFINIR\"]=\"TARJETAS\"\n",
    "keys[\"TARJETA_EMP_COMPRA\"]=\"TARJETA_EMP\"\n",
    "keys[\"TARJETA_EMP_EFECTIVO\"]=\"TARJETA_EMP\"\n",
    "keys[\"TARJETA_EMP_OTROS_CONCEPTOS\"]=\"TARJETA_EMP\"\n",
    "keys[\"TARJETA_EMP_SIN_DEFINIR\"]=\"TARJETA_EMP\"\n",
    "keys[\"VEHICULAR\"]=\"PRESTAMOS\"\n",
    "\n",
    "def normalizar_producto_rcc(text):\n",
    "  text = str(text).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\").replace(\" \",\"_\").upper()\n",
    "  return keys[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ligMCnBpC0iX"
   },
   "outputs": [],
   "source": [
    "# Empiezo creando un agrupado por mes, con la suma total de cada mes \n",
    "# y ademas la cantidad de dias distintos\n",
    "rcc[\"producto2\"] = list(map(lambda p: normalizar_producto_rcc(p), rcc[\"producto\"]))\n",
    "#rcc.groupby([\"producto2\"])[[\"id_persona\"]].count()\n",
    "rcc.groupby([\"producto\"])[[\"id_persona\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtPN7ibEOxmV"
   },
   "source": [
    "## Base de Campanias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Moj8XvOwWL"
   },
   "outputs": [],
   "source": [
    "print(campanias.shape)\n",
    "campanias.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuATjbocZlYW"
   },
   "outputs": [],
   "source": [
    "campanias.groupby([\"codmes\"])[[\"id_persona\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On9YCrvBFqOL"
   },
   "outputs": [],
   "source": [
    "def normalizar_producto(text):\n",
    "  text = str(text).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\").replace(\" \",\"_\").upper()\n",
    "  if \"SEGURO\" in text:\n",
    "    return \"SEGURO\"\n",
    "  else:  \n",
    "    if \"PRSTAMO\" in text or \"PRESTAMO\" in text or \"PRÉSTAMO\" in text or \"CRDITO\" in text:\n",
    "      return \"PRESTAMO\"\n",
    "    else:    \n",
    "      if (\"_TC\" in text or \"TC_\" in text) and \"ADQUISICIN_TC\" not in text:\n",
    "        return \"OTROS_TC\"\n",
    "      else:    \n",
    "        r = text\n",
    "        r = r.split(\"_\")\n",
    "        if len(r) > 1:\n",
    "          return r[0] + \"_\" + r[1]\n",
    "        else:\n",
    "          return r[0]\n",
    "\n",
    "def normalizar_canal(text):\n",
    "  text = str(text).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\").replace(\" \",\"_\").replace(\"(\",\"_\").replace(\")\",\"_\").upper()  \n",
    "  r = text\n",
    "  if \"ABP\" in r:\n",
    "    return \"ABP\"\n",
    "  else:  \n",
    "    if \"ATM\" in r:\n",
    "      return \"ATM\"\n",
    "    else:\n",
    "      if \"BANCA_TELEFONICA\" in r:\n",
    "        return \"BANCA_TELEFONICA\"\n",
    "      else:  \n",
    "        if \"BOLSA\" in r:\n",
    "          return \"BOLSA\"\n",
    "        else:\n",
    "          if \"CALL_EXTERNO\" in r:\n",
    "            return \"CALL_EXTERNO\"\n",
    "          else:  \n",
    "            if \"CARTERA\" in r:\n",
    "              return \"CARTERA\"\n",
    "            else:\n",
    "              if \"EBP\" in r:\n",
    "                return \"EBP\"\n",
    "              else:  \n",
    "                if \"EXPRESS\" in r:\n",
    "                  return \"EXPRESS\"\n",
    "                else:\n",
    "                  if \"FFVV\" in r:\n",
    "                    return \"FFVV\"\n",
    "                  else:  \n",
    "                    if \"GT\" in r:\n",
    "                      return \"GT\"\n",
    "                    else:\n",
    "                      if \"RED_DE_TIENDAS\" in r:\n",
    "                        return \"RED_DE_TIENDAS\"\n",
    "                      else:  \n",
    "                        if \"TELEVENTAS\" in r:\n",
    "                          return \"TELEVENTAS\"\n",
    "                        else:\n",
    "                          if \"TLV\" in r:\n",
    "                            return \"TLV\"\n",
    "                          else:  \n",
    "                            if \"NAN\" in r:\n",
    "                              return \"NAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-clrATLSWAL"
   },
   "outputs": [],
   "source": [
    "# Empiezo creando un agrupado por mes, con la suma total de cada mes \n",
    "# y ademas la cantidad de dias distintos\n",
    "#campanias[\"month\"] = list(map(lambda datee: datetime(int(str(datee)[:4]), int(str(datee)[4:6]), 1), campanias[\"codmes\"]))\n",
    "campanias[\"producto\"] = list(map(lambda p: normalizar_producto(p), campanias[\"producto\"]))\n",
    "campanias[\"canal_asignado\"] = list(map(lambda c: normalizar_canal(c), campanias[\"canal_asignado\"]))\n",
    "campanias[\"prod_canal\"] = list(map(lambda p, c: str(p)+\"_\"+str(c), campanias[\"producto\"], campanias[\"canal_asignado\"]))\n",
    "campanias.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LwOaktJ7yqh"
   },
   "source": [
    "## Creación del Target de predicción\n",
    "\n",
    "Se opta por construir un target binario, para establecer quienes son clientes rentables y, por tanto, es conveniente hacerles campaña para atraerlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1unMV0hd7yqh"
   },
   "outputs": [],
   "source": [
    "global X_train\n",
    "global y_train\n",
    "global X_test\n",
    "global y_test\n",
    "\n",
    "y_train = train[['codmes', 'id_persona', 'margen']].copy()\n",
    "y_train[\"prediction_id\"] = y_train[\"id_persona\"].astype(str) + \"_\" + y_train[\"codmes\"].astype(str)\n",
    "y_train[\"target\"] = (y_train[\"margen\"] > 0).astype(int)\n",
    "y_train = y_train.set_index(\"prediction_id\")\n",
    "X_train = train.drop([\"codtarget\", \"margen\"], axis=1)\n",
    "X_train[\"prediction_id\"] = X_train[\"id_persona\"].astype(str) + \"_\" + X_train[\"codmes\"].astype(str)\n",
    "X_test[\"prediction_id\"] = X_test[\"id_persona\"].astype(str) + \"_\" + X_test[\"codmes\"].astype(str)\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXwqLvNT7yqk"
   },
   "source": [
    "## Consolidación de Bases\n",
    "\n",
    "Se unene todas las bases por id_persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnItxE1K7yqk"
   },
   "outputs": [],
   "source": [
    "sunat = sunat.groupby([\"id_persona\", \"activ_econo\"]).meses_alta.sum().unstack(level=1, fill_value=0).astype(\"int32\")\n",
    "vehicular1 = vehicular.groupby([\"id_persona\", \"marca\"]).veh_var1.sum().unstack(level=1, fill_value=0).astype(\"float32\")\n",
    "vehicular2 = vehicular.groupby([\"id_persona\", \"marca\"]).veh_var2.sum().unstack(level=1, fill_value=0).astype(\"float32\")\n",
    "reniec = reniec.set_index(\"id_persona\").astype(\"float32\")\n",
    "del vehicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ih2PVw8J7yqm"
   },
   "outputs": [],
   "source": [
    "vehicular1.columns = [c + \"_v1\" for c in vehicular1.columns]\n",
    "vehicular2.columns = [c + \"_v2\" for c in vehicular2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eagq27T7Ogap"
   },
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7FenT58ePVdT"
   },
   "outputs": [],
   "source": [
    "X_train[\"month\"] = list(map(lambda datee: datetime(int(str(datee)[:4]), int(str(datee)[4:6]), 1), X_train[\"codmes\"]))\n",
    "#X_train = X_train.set_index(\"prediction_id\").astype(\"int32\").reset_index().set_index(\"id_persona\")\n",
    "X_train = X_train.merge(digital_hist, how=\"left\", on=[\"id_persona\",\"month\"])\n",
    "X_train = X_train.merge(dt_vehiculos, how=\"left\", on=[\"id_persona\"])\n",
    "X_train = X_train.merge(reniec, how=\"left\", on=[\"id_persona\"])\n",
    "X_train = X_train.merge(sunat, how=\"left\", on=[\"id_persona\"])\n",
    "X_train.drop([\"month\"], axis=1, inplace=True)\n",
    "\n",
    "X_test[\"month\"] = list(map(lambda datee: datetime(int(str(datee)[:4]), int(str(datee)[4:6]), 1), X_test[\"codmes\"]))\n",
    "#X_test = X_test.set_index(\"prediction_id\").astype(\"int32\").reset_index().set_index(\"id_persona\")\n",
    "X_test = X_test.merge(digital_hist, how=\"left\", on=[\"id_persona\",\"month\"])\n",
    "X_test = X_test.merge(dt_vehiculos, how=\"left\", on=[\"id_persona\"])\n",
    "X_test = X_test.merge(reniec, how=\"left\", on=[\"id_persona\"])\n",
    "X_test = X_test.merge(sunat, how=\"left\", on=[\"id_persona\"])\n",
    "X_test.drop([\"month\"], axis=1, inplace=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OTlWbih7yqp"
   },
   "outputs": [],
   "source": [
    "del vehicular1, vehicular2, reniec, sunat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9vIZBZIPeYi"
   },
   "source": [
    "## Variables historicas de campañas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzfIcjds7yqr"
   },
   "outputs": [],
   "source": [
    "camp_canal = campanias.groupby([\"codmes\", \"id_persona\", \"canal_asignado\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\n",
    "camp_prod = campanias.groupby([\"codmes\", \"id_persona\", \"producto\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\n",
    "del campanias\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIbe8QOM7yqt"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7RrBgue7yqw"
   },
   "outputs": [],
   "source": [
    "meses = {\n",
    "    201901: slice(201801, 201810),\n",
    "    201902: slice(201802, 201811),\n",
    "    201903: slice(201803, 201812),\n",
    "    201904: slice(201804, 201901),\n",
    "    201905: slice(201805, 201902),\n",
    "    201906: slice(201806, 201903),\n",
    "    201907: slice(201807, 201904)\n",
    "}\n",
    "tw=\"10M\"\n",
    "complementos = []\n",
    "for mes in meses.keys():\n",
    "    print(\"*\"*10, mes, \"*\"*10)\n",
    "    \n",
    "    res = pd.concat([camp_prod.loc[meses[mes]].groupby(\"id_persona\").sum()], axis=1)\n",
    "    res[\"codmes\"] = mes\n",
    "    res = res.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res.columns = [tw+\"_sum_\"+c for c in res.columns]\n",
    "\n",
    "    res_c = pd.concat([camp_canal.loc[meses[mes]].groupby(\"id_persona\").sum()], axis=1)\n",
    "    res_c[\"codmes\"] = mes\n",
    "    res_c = res_c.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res_c.columns = [tw+\"_sum_c\"+c for c in res_c.columns]\n",
    "    \n",
    "    res2 = pd.concat([camp_prod.loc[meses[mes]].groupby(\"id_persona\").mean()], axis=1)\n",
    "    res2[\"codmes\"] = mes\n",
    "    res2 = res2.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res2.columns = [tw+\"_avg_\"+c for c in res2.columns]\n",
    "\n",
    "    res2_c = pd.concat([camp_canal.loc[meses[mes]].groupby(\"id_persona\").mean()], axis=1)\n",
    "    res2_c[\"codmes\"] = mes\n",
    "    res2_c = res2_c.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res2_c.columns = [tw+\"_avg_c\"+c for c in res2_c.columns]\n",
    "\n",
    "    res3 = pd.concat([camp_prod.loc[meses[mes]].groupby(\"id_persona\").max()], axis=1)\n",
    "    res3[\"codmes\"] = mes\n",
    "    res3 = res3.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res3.columns = [tw+\"_max_\"+c for c in res3.columns]\n",
    "\n",
    "    res3_c = pd.concat([camp_canal.loc[meses[mes]].groupby(\"id_persona\").max()], axis=1)\n",
    "    res3_c[\"codmes\"] = mes\n",
    "    res3_c = res3_c.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "    res3_c.columns = [tw+\"_max_c\"+c for c in res3_c.columns]\n",
    "\n",
    "    res = pd.concat([res, res2, res3, res_c, res2_c, res3_c], axis=1)\n",
    "\n",
    "    complementos.append(res)\n",
    "\n",
    "gc.collect()\n",
    "print(\"contatenando complementos \"+str(tw))\n",
    "complementos = pd.concat(complementos)\n",
    "gc.collect()\n",
    "print(\"X_train join\")\n",
    "X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "gc.collect()\n",
    "print(\"X_test join\")\n",
    "X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "gc.collect()\n",
    "\n",
    "del camp_canal, camp_prod, complementos,res\n",
    "gc.collect()\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDOC-95G7yqy"
   },
   "source": [
    "## Renombrado de Variables con nombre no ascii\n",
    "\n",
    "El algoritmo que usamos no se lleva bien con cadenas de texto con caracteres especiales, las renombramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7UShdBM7yq1"
   },
   "outputs": [],
   "source": [
    "def emprolijar_dataset():\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "  \n",
    "  non_ascii = X_train.columns[[not all(ord(c) < 128 for c in s) for s in X_train.columns]].tolist()\n",
    "  for i, c in enumerate(non_ascii):\n",
    "      X_train[\"non_ascii_\" + str(i)] = X_train[c]\n",
    "      X_train = X_train.drop(c, axis= 1)\n",
    "      X_test[\"non_ascii_\" + str(i)] = X_test[c]\n",
    "      X_test = X_test.drop(c, axis= 1)\n",
    "  \n",
    "  X_train=X_train.dropna(axis=1,how='all')\n",
    "  X_test=X_test.dropna(axis=1,how='all')\n",
    "\n",
    "  vars = set(X_train.columns).intersection(set(X_test.columns))\n",
    "\n",
    "  X_test = X_test[vars]\n",
    "  X_train = X_train[vars]\n",
    "\n",
    "  X_train.fillna(0)\n",
    "  X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPCxsHDKoFbC"
   },
   "source": [
    "## Variables historicas de RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gFKrLT4oUTS"
   },
   "outputs": [],
   "source": [
    "global rcc_prod\n",
    "global rcc_prod2\n",
    "global rcc_tot\n",
    "global rcc_bco\n",
    "global rcc_sdo_prod\n",
    "global rcc_sdo_prod2\n",
    "global rcc_sdo_tot\n",
    "global rcc_sdo_bco\n",
    "global rcc_mora_prod\n",
    "global rcc_mora_prod2\n",
    "global rcc_mora_tot\n",
    "global rcc_mora_bco\n",
    "\n",
    "rcc_prod = rcc.groupby([\"codmes\", \"id_persona\", \"producto\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\n",
    "rcc_prod2 = rcc.groupby([\"codmes\", \"id_persona\", \"producto2\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\n",
    "rcc_tot = rcc.groupby([\"codmes\", \"id_persona\"]).size().reset_index().set_index([\"codmes\",\"id_persona\"]).sort_index().astype(\"int32\")\n",
    "rcc_bco = rcc.groupby([\"codmes\", \"id_persona\", \"BN_mto_saldo\"]).size().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(\"int32\")\n",
    "\n",
    "rcc_sdo_prod = rcc.groupby([\"codmes\", \"id_persona\", \"producto\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_sdo_prod2 = rcc.groupby([\"codmes\", \"id_persona\", \"producto2\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_sdo_tot = rcc.groupby([\"codmes\", \"id_persona\"])[\"mto_saldo\"].sum().reset_index().set_index([\"codmes\",\"id_persona\"]).sort_index().astype(float)\n",
    "rcc_sdo_bco = rcc.groupby([\"codmes\", \"id_persona\", \"BN_mto_saldo\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_sdo_clasif = rcc.groupby([\"codmes\", \"id_persona\", \"clasif\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_sdo_mora = rcc.groupby([\"codmes\", \"id_persona\", \"rango_mora\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "\n",
    "rcc_mora_prod = rcc.groupby([\"codmes\", \"id_persona\", \"producto\"])[\"rango_mora\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_mora_prod2 = rcc.groupby([\"codmes\", \"id_persona\", \"producto2\"])[\"rango_mora\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_mora_tot = rcc.groupby([\"codmes\", \"id_persona\"])[\"rango_mora\"].max().reset_index().set_index([\"codmes\",\"id_persona\"]).sort_index().astype(float)\n",
    "rcc_mora_bco = rcc.groupby([\"codmes\", \"id_persona\", \"BN_mto_saldo\"])[\"rango_mora\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAmVyKOh8W5M"
   },
   "source": [
    "### 3 Meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2G499P9mAf8u"
   },
   "outputs": [],
   "source": [
    "def agregar_rcc_3m(FEATURE_SELECTION):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  meses = {\n",
    "      201901: slice(201807, 201809),\n",
    "      201902: slice(201808, 201810),\n",
    "      201903: slice(201809, 201811),\n",
    "      201904: slice(201810, 201812),\n",
    "      201905: slice(201811, 201901),\n",
    "      201906: slice(201812, 201902),\n",
    "      201907: slice(201901, 201903)\n",
    "  }\n",
    "  tw=\"3M_RCC_\"\n",
    "  reses=[]\n",
    "  dts = dict()\n",
    "  dts[\"sz_prod\"] = rcc_prod\n",
    "  dts[\"sz_tot\"] = rcc_tot\n",
    "  dts[\"sz_bco\"] = rcc_bco\n",
    "  dts[\"sdo_prod\"] = rcc_sdo_prod\n",
    "  dts[\"sdo_tot\"] = rcc_sdo_tot\n",
    "  dts[\"sdo_bco\"] = rcc_sdo_bco\n",
    "  #dts[\"sdo_mora\"] = rcc_sdo_mora \n",
    "\n",
    "  complementos = []\n",
    "\n",
    "  print(\"agregar_rcc_3m() - INICIO\")\n",
    "\n",
    "  for mes in meses.keys():\n",
    "      print(\" \"*5 + \"Mes: \"+str(mes))\n",
    "      reses = []\n",
    "\n",
    "      for dd in dts.keys():\n",
    "        res = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").sum()], axis=1)\n",
    "        res[\"codmes\"] = mes\n",
    "        res = res.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res.columns = [tw+\"_sum_\"+dd+\"_\"+str(c) for c in res.columns]\n",
    "        reses.append(copy.copy(res))\n",
    "        \n",
    "        res2 = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").mean()], axis=1)\n",
    "        res2[\"codmes\"] = mes\n",
    "        res2 = res2.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res2.columns = [tw+\"_avg_\"+dd+\"_\"+str(c) for c in res2.columns]\n",
    "        reses.append(copy.copy(res2))\n",
    "\n",
    "      res = pd.concat(reses, axis=1)\n",
    "\n",
    "      complementos.append(res)\n",
    "\n",
    "  complementos = pd.concat(complementos)\n",
    "  gc.collect()\n",
    "  \n",
    "  if not(FEATURE_SELECTION):\n",
    "    X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")    \n",
    "    try:\n",
    "      X_train = X_train.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww =0\n",
    "\n",
    "    X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "\n",
    "    try:\n",
    "      X_test = X_test.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww = 0\n",
    "    gc.collect()\n",
    "\n",
    "  else:\n",
    "    cols_to_hang = X_train.columns\n",
    "    cols_to_rmv = [ x for x in X_train.columns if 'codmes' not in x and 'id_persona' not in x ]\n",
    "    X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "    X_train_fe = X_train.drop(cols_to_rmv, axis=1)\n",
    "    X_train_fe.fillna(0)\n",
    "    gc.collect()\n",
    "\n",
    "    # Scoring features focused in the conversion target\n",
    "    print(\"     Feature Selection [ \", end = '')\n",
    "    final_iv, scored_fe = cff.score_features(X_train_fe, y_train[\"target\"])\n",
    "    scored_fe   = scored_fe.rename(columns={'VAR_NAME':'index'})\n",
    "    selected_fe = scored_fe.sort_values(['IV'],ascending=0).head(50)\n",
    "    selected_fe = selected_fe.set_index('index').index\n",
    "    print(\" ]\")\n",
    "\n",
    "    # Always we need the index columns to do the join after\n",
    "    col_index = pd.Index([\"codmes\",\"id_persona\"]) \n",
    "    col_index = col_index.append(selected_fe)\n",
    "    col_index = col_index.append(cols_to_hang)\n",
    "    col_index = list(set(col_index))\n",
    "\n",
    "    # At the end we keep just the selected columns and the index columns\n",
    "    X_train = X_train[col_index]\n",
    "\n",
    "    try:\n",
    "      X_train = X_train.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww =0\n",
    "\n",
    "    X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "    X_test = X_test[col_index]\n",
    "\n",
    "    try:\n",
    "      X_test = X_test.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww = 0\n",
    "\n",
    "  gc.collect()\n",
    "\n",
    "  emprolijar_dataset()\n",
    "\n",
    "  print(\"agregar_rcc_3m() - FIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ttu3-RBR8Tvq"
   },
   "source": [
    "### 10 Meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGyp7j-RoJIj"
   },
   "outputs": [],
   "source": [
    "def agregar_rcc_10m(FEATURE_SELECTION):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "  meses = {\n",
    "      201901: slice(201712, 201809),\n",
    "      201902: slice(201801, 201810),\n",
    "      201903: slice(201802, 201811),\n",
    "      201904: slice(201803, 201812),\n",
    "      201905: slice(201804, 201901),\n",
    "      201906: slice(201805, 201902),\n",
    "      201907: slice(201806, 201903)\n",
    "  }\n",
    "  tw=\"10M_RCC_\"\n",
    "  reses=[]\n",
    "  dts = dict()\n",
    "  dts[\"sz_prod\"] = rcc_prod\n",
    "  dts[\"sz_prod2\"] = rcc_prod2\n",
    "  dts[\"sz_tot\"] = rcc_tot\n",
    "  dts[\"sz_bco\"] = rcc_bco\n",
    "  \n",
    "  dts[\"sdo_prod\"] = rcc_sdo_prod\n",
    "  dts[\"sdo_prod2\"] = rcc_sdo_prod2\n",
    "  dts[\"sdo_tot\"] = rcc_sdo_tot\n",
    "  dts[\"sdo_bco\"] = rcc_sdo_bco\n",
    "  #dts[\"sdo_clasif\"] = rcc_sdo_clasif\n",
    "  #dts[\"sdo_mora\"] = rcc_sdo_mora \n",
    "\n",
    "  dts[\"mora_prod\"] = rcc_mora_prod\n",
    "  dts[\"mora_prod2\"] = rcc_mora_prod2\n",
    "  dts[\"mora_tot\"] = rcc_mora_tot\n",
    "  dts[\"mora_bco\"] = rcc_mora_bco\n",
    "  complementos = []\n",
    "  print(\"agregar_rcc_10m() - INICIO\")\n",
    "  for mes in meses.keys():\n",
    "      print(\" \"*5 + \"Mes: \"+str(mes))\n",
    "      reses = []\n",
    "\n",
    "      for dd in dts.keys():\n",
    "        res = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").sum()], axis=1)\n",
    "        res[\"codmes\"] = mes\n",
    "        res = res.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res.columns = [tw+\"_sum_\"+dd+\"_\"+str(c) for c in res.columns]\n",
    "        reses.append(copy.copy(res))\n",
    "        \n",
    "        res2 = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").mean()], axis=1)\n",
    "        res2[\"codmes\"] = mes\n",
    "        res2 = res2.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res2.columns = [tw+\"_avg_\"+dd+\"_\"+str(c) for c in res2.columns]\n",
    "        reses.append(copy.copy(res2))\n",
    "\n",
    "        res3 = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").max()], axis=1)\n",
    "        res3[\"codmes\"] = mes\n",
    "        res3 = res3.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res3.columns = [tw+\"_max_\"+dd+\"_\"+str(c) for c in res3.columns]\n",
    "        reses.append(copy.copy(res3))\n",
    "\n",
    "        res4 = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").size()], axis=1)\n",
    "        res4[\"codmes\"] = mes\n",
    "        res4 = res4.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res4.columns = [tw+\"_dis_\"+dd+\"_\"+str(c) for c in res4.columns]\n",
    "        reses.append(copy.copy(res4))\n",
    "\n",
    "      res = pd.concat(reses, axis=1)\n",
    "\n",
    "      complementos.append(res)\n",
    "\n",
    "  gc.collect()\n",
    "  complementos = pd.concat(complementos)\n",
    "  try:\n",
    "    X_train = X_train.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    ww = 0\n",
    "    \n",
    "  if not(FEATURE_SELECTION):\n",
    "    X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")    \n",
    "    try:\n",
    "      X_train = X_train.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww =0\n",
    "\n",
    "    X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "\n",
    "    try:\n",
    "      X_test = X_test.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww = 0\n",
    "    gc.collect()\n",
    "    \n",
    "  else:\n",
    "    cols_to_hang = X_train.columns\n",
    "    cols_to_rmv = [ x for x in X_train.columns if 'codmes' not in x and 'id_persona' not in x ]\n",
    "    X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "    X_train_fe = X_train.drop(cols_to_rmv, axis=1)\n",
    "    X_train_fe.fillna(0)\n",
    "    gc.collect()\n",
    "\n",
    "    # Scoring features focused in the conversion target\n",
    "    print(\"     Feature Selection [ \", end = '')\n",
    "    final_iv, scored_fe = cff.score_features(X_train_fe, y_train[\"target\"])\n",
    "    scored_fe   = scored_fe.rename(columns={'VAR_NAME':'index'})\n",
    "    selected_fe = scored_fe.sort_values(['IV'],ascending=0).head(200)\n",
    "    selected_fe = selected_fe.set_index('index').index\n",
    "    print(\" ]\")\n",
    "\n",
    "    # Always we need the index columns to do the join after\n",
    "    col_index = pd.Index([\"codmes\",\"id_persona\"]) \n",
    "    col_index = col_index.append(selected_fe)\n",
    "    col_index = col_index.append(cols_to_hang)\n",
    "    col_index = list(set(col_index))\n",
    "\n",
    "    # At the end we keep just the selected columns and the index columns\n",
    "    X_train = X_train[col_index]\n",
    "\n",
    "    try:\n",
    "      X_train = X_train.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww = 0\n",
    "\n",
    "    X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "    X_test = X_test[col_index]\n",
    "\n",
    "    try:\n",
    "      X_test = X_test.drop([\"index\"], axis=1)\n",
    "    except:\n",
    "      ww = 0\n",
    "\n",
    "  gc.collect()\n",
    "\n",
    "  emprolijar_dataset()\n",
    "\n",
    "  print(\"agregar_rcc_10m() - FIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-OdRVArW9m0"
   },
   "source": [
    "## Comparacion de limite con otros bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6iLavvfJWIp"
   },
   "outputs": [],
   "source": [
    "# Tomo solo los items que representan el limite de TC y consumo TC en otros bancos\n",
    "rcc_tarj = rcc[rcc.producto.isin([\"TARJETAS COMPRAS\",\"TARJETAS EFECTIVO\",\"LINEA TOTAL TC\"])]\n",
    "\n",
    "# Unifico compras y extraccion de efectivo en un solo item\n",
    "rcc_tarj[\"producto\"] = list(map(lambda b: \"TARJETAS COMPRAS\" if \"TARJETAS EFECTIVO\" in b else b, rcc_tarj[\"producto\"]))\n",
    "\n",
    "# Agrupo por cliente, mes y producto tomando el maximo y la suma total de todos los bancos\n",
    "rcc_sum_prod = rcc_tarj.groupby([\"codmes\", \"id_persona\", \"producto\"])[\"mto_saldo\"].sum().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "rcc_max_prod = rcc_tarj.groupby([\"codmes\", \"id_persona\", \"producto\"])[\"mto_saldo\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(float)\n",
    "\n",
    "# Calculo para cada cliente, el % de uso de su limite en otros bancos\n",
    "rcc_max_prod[\"uso_lim\"] = list(map(lambda uso, lim: round(10*(uso/lim)) if uso >0 and lim > 0 else 100 if uso >0 and lim == 0 else 0 if lim > 0 else -1, rcc_max_prod[\"TARJETAS COMPRAS\"], rcc_max_prod[\"LINEA TOTAL TC\"]))\n",
    "rcc_sum_prod[\"uso_lim\"] = list(map(lambda uso, lim: round(10*(uso/lim)) if uso >0 and lim > 0 else 100 if uso >0 and lim == 0 else 0 if lim > 0 else -1, rcc_sum_prod[\"TARJETAS COMPRAS\"], rcc_sum_prod[\"LINEA TOTAL TC\"]))\n",
    "\n",
    "rcc_max_prod.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjeCnDa9hj2i"
   },
   "outputs": [],
   "source": [
    "# Linea maxima por grupo de banco\n",
    "rcc_lim_tarj = rcc_tarj[rcc_tarj.producto.isin([\"LINEA TOTAL TC\"])]\n",
    "rcc_lim_max_bco = rcc_lim_tarj.groupby([\"codmes\", \"id_persona\", \"BN_mto_saldo\"])[\"mto_saldo\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(int)\n",
    "rcc_lim_max_bco.columns = [\"id_persona\",\"lim_bco0\",\"lim_bco1\",\"lim_bco2\",\"lim_bco3\"]\n",
    "del rcc_lim_tarj\n",
    "rcc_lim_max_bco.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unBvANdNk1TP"
   },
   "outputs": [],
   "source": [
    "# Consumo maximo por grupo de banco\n",
    "rcc_cons_tarj = rcc_tarj[rcc_tarj.producto.isin([\"TARJETAS COMPRAS\"])]\n",
    "rcc_cons_max_bco = rcc_cons_tarj.groupby([\"codmes\", \"id_persona\", \"BN_mto_saldo\"])[\"mto_saldo\"].max().unstack(level=2, fill_value=0).reset_index().set_index(\"codmes\").sort_index().astype(int)\n",
    "rcc_cons_max_bco.columns = [\"id_persona\",\"cons_bco0\",\"cons_bco1\",\"cons_bco2\",\"cons_bco3\"]\n",
    "del rcc_cons_tarj\n",
    "del rcc_tarj\n",
    "rcc_cons_max_bco.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTLrlawRXBzS"
   },
   "outputs": [],
   "source": [
    "def linea_vs_otros_bancos(meses, tw, precision):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  dts = dict()\n",
    "  dts[\"sum_prod\"] = rcc_sum_prod\n",
    "  dts[\"mx_prod\"] = rcc_max_prod\n",
    "  dts[\"lim_mx_bco\"] = rcc_lim_max_bco\n",
    "  #dts[\"cons_mx_bco\"] = rcc_cons_max_bco\n",
    "\n",
    "  complementos = []\n",
    "  for mes in meses.keys():\n",
    "      reses = []\n",
    "\n",
    "      for dd in dts.keys():\n",
    "        res = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").max()], axis=1)\n",
    "        res[\"codmes\"] = mes\n",
    "        res = res.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res.columns = [tw+\"_max_\"+dd+\"_\"+str(c) for c in res.columns]\n",
    "        reses.append(copy.copy(res))\n",
    "        \n",
    "        res2 = pd.concat([dts[dd].loc[meses[mes]].groupby(\"id_persona\").mean()], axis=1)\n",
    "        res2[\"codmes\"] = mes\n",
    "        res2 = res2.reset_index().set_index([\"id_persona\", \"codmes\"]).astype(\"float32\")\n",
    "        res2.columns = [tw+\"_avg_\"+dd+\"_\"+str(c) for c in res2.columns]\n",
    "        reses.append(copy.copy(res2))\n",
    "\n",
    "      res = pd.concat(reses, axis=1)\n",
    "\n",
    "      complementos.append(res)\n",
    "\n",
    "  gc.collect()\n",
    "  complementos = pd.concat(complementos)\n",
    "  X_train = X_train.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "  try:\n",
    "    X_train = X_train.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    ww = 0\n",
    "\n",
    "  X_test = X_test.reset_index().join(complementos, on=[\"id_persona\", \"codmes\"]).set_index(\"prediction_id\")\n",
    "  try:\n",
    "    X_test = X_test.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    ww = 0\n",
    "\n",
    "  # Relleno con ceros los casos donde no se encontro datos\n",
    "  X_train.fillna(0)\n",
    "  X_test.fillna(0)\n",
    "  gc.collect()\n",
    "\n",
    "  # Por cada nueva columna creada, creo el cociente en training y testing y luego borro las variables usadas para calcularlo  \n",
    "  for c in X_train.columns:\n",
    "      if (tw in c):\n",
    "        if c in X_test.columns:\n",
    "          if 'LINEA TOTAL TC' in c or 'TARJETAS COMPRAS' in c:\n",
    "            X_train[\"linea_ofrecida_\"+c] = list(map(lambda ofre, actual: round(precision*cff.tendencia(ofre, actual)), X_train[\"linea_ofrecida\"], X_train[c]))\n",
    "            X_train.fillna(0)\n",
    "            X_test[\"linea_ofrecida_\"+c] = list(map(lambda ofre, actual: round(precision*cff.tendencia(ofre, actual)), X_test[\"linea_ofrecida\"], X_test[c]))\n",
    "            X_test.fillna(0)\n",
    "\n",
    "        # Elimino las variables agregadas, para dejar solo los cocientes\n",
    "        if 'LINEA TOTAL TC' in c or 'TARJETAS COMPRAS' in c:\n",
    "          X_train = X_train.drop([c], axis=1)\n",
    "        \n",
    "        # La borro de testing solo si existe\n",
    "        if c in X_test.columns:\n",
    "          if 'LINEA TOTAL TC' in c or 'TARJETAS COMPRAS' in c:\n",
    "            X_test = X_test.drop([c], axis=1)\n",
    "\n",
    "def comparar_linea_otros_bancos(precision):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  print(\"comparar_linea_otros_bancos() - INICIO\")\n",
    "  \n",
    "  #meses = {201901: slice(201809, 201809),201902: slice(201810, 201810),201903: slice(201811, 201811),201904: slice(201812, 201812),201905: slice(201901, 201901),201906: slice(201902, 201902),201907: slice(201903, 201903)}\n",
    "  #tw=\"1M_RCCTARJ_\"\n",
    "  #linea_vs_otros_bancos(meses, tw, precision)\n",
    "\n",
    "  meses = {201901: slice(201807, 201809),201902: slice(201808, 201810),201903: slice(201809, 201811),201904: slice(201810, 201812),201905: slice(201811, 201901),201906: slice(201812, 201902),201907: slice(201901, 201903)}\n",
    "  tw=\"3M_RCCTARJ_\"\n",
    "  linea_vs_otros_bancos(meses, tw, precision)\n",
    "  \n",
    "  meses = {201901: slice(201712, 201809),201902: slice(201801, 201810),201903: slice(201802, 201811),201904: slice(201803, 201812),201905: slice(201804, 201901),201906: slice(201805, 201902),      201907: slice(201806, 201903)}\n",
    "  tw=\"10M_RCCTARJ_\"\n",
    "  linea_vs_otros_bancos(meses, tw, precision)\n",
    "\n",
    "  emprolijar_dataset()\n",
    "\n",
    "  print(\"comparar_linea_otros_bancos() - FIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Cndza8SL2yW"
   },
   "source": [
    "## Tendencia historica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZ_M3-vJL6-d"
   },
   "outputs": [],
   "source": [
    "def var_tend_hist():\n",
    "  trcols = set([x.replace('3M_', '') for x in X_train.columns if '3M_RCC' in x])\n",
    "  trcols = trcols.intersection(set([x.replace('10M_', '') for x in X_train.columns if '10M_RCC' in x]))\n",
    "  tecols = set([x.replace('3M_', '') for x in X_test.columns if '3M_RCC' in x])\n",
    "  tecols = tecols.intersection(set([x.replace('10M_', '') for x in X_test.columns if '10M_RCC' in x]))\n",
    "  cols_to_apply = trcols.intersection(tecols)\n",
    "\n",
    "  for c in cols_to_apply:\n",
    "    for data_temp in [X_train, X_test]:\n",
    "      # Normalizo las variables antes de calcular la tendencia\n",
    "      cff.scale_feature(data_temp, [\"10M_\"+c , \"3M_\"+c ], 200, preprocessing.MaxAbsScaler())\n",
    "\n",
    "      #Calculo la tendencia actual+1 vs +2+3\n",
    "      data_temp[\"td_\"+c]= list(map(lambda t2, t1: cff.tendencia(t1, t2), data_temp[\"SCL_10M_\"+c], data_temp[\"SCL_3M_\"+c]))\n",
    "      \n",
    "      delete=pd.DataFrame(data_temp.filter(like='SCL_').columns).set_index(0).index\n",
    "      data_temp=data_temp.drop(delete,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shbimgiwq5QC"
   },
   "source": [
    "# LGBMCLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vsb5YyBH7yq5"
   },
   "outputs": [],
   "source": [
    "def aplicar_LGBMClassfier(indice, kseeds, ENTRENAR_TODOS_MESES, k_folds):\n",
    "\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "  global Xt\n",
    "  global yt\n",
    "  global wt\n",
    "  global Xv\n",
    "  global yv\n",
    "  global wv\n",
    "  global learner\n",
    "  global flag\n",
    "  flag = 1\n",
    "\n",
    "  def evaluar_ganancia(y_true, y_pred):\n",
    "    global flag\n",
    "    if flag == 1:\n",
    "      optimization2 = sum(y_pred * wt)\n",
    "      flag = 0\n",
    "    else:\n",
    "      optimization2 = sum(y_pred * wv)\n",
    "      flag = 1\n",
    "\n",
    "    return 'GAIN', optimization2, True\n",
    "    \n",
    "  drop_cols = [\"codmes\",\"id_persona\"]\n",
    "  fi = []\n",
    "  test_probs = []\n",
    "  train_probs2 = []\n",
    "  #for s in kseeds:\n",
    "  train_probs = []\n",
    "  s = 20191125\n",
    "  #print(\"Seed: \"+str(s))\n",
    "  #for mes in X_train.codmes.unique():\n",
    "      #print(\"   Mes: \"+str(mes))\n",
    "      #Xt = X_train[X_train.codmes != mes]\n",
    "      #yt = y_train.loc[Xt.index, \"target\"]\n",
    "      #Xt = Xt.drop(drop_cols, axis=1)    \n",
    "      #wt = y_train.loc[Xt.index, \"margen\"]\n",
    "\n",
    "      #Xv = X_train[X_train.codmes == mes]\n",
    "      #yv = y_train.loc[Xv.index, \"target\"]\n",
    "      #wv = y_train.loc[Xv.index, \"margen\"]\n",
    "\n",
    "      #learner = LGBMClassifier(n_estimators=10000, random_state=s, objective=\"binary\", metric=['GAIN'])\n",
    "      #learner.fit(Xt, yt, early_stopping_rounds=10, eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=0, eval_metric=evaluar_ganancia)\n",
    "      \n",
    "      #if not(ENTRENAR_TODOS_MESES):\n",
    "          #test_probs.append(pd.Series(learner.predict_proba(X_test.drop(drop_cols, axis=1))[:, -1],index=X_test.index, name=\"fold_\" + str(mes)))\n",
    "          #train_probs2.append(pd.Series(learner.predict_proba(Xv.drop(drop_cols, axis=1))[:, -1], index=Xv.index, name=\"fold_\" + str(mes)))\n",
    "\n",
    "      #train_probs.append(pd.Series(learner.predict_proba(Xv.drop(drop_cols, axis=1))[:, -1],index=Xv.index, name=\"probs\"))\n",
    "      #fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "  if ENTRENAR_TODOS_MESES:  \n",
    "    i = 0\n",
    "    for train_idx, valid_idx in model_selection.StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=s).split(X_train, y_train[\"target\"]):\n",
    "      i += 1\n",
    "\n",
    "      # Split the train & validation datasets with the cv fold ids\n",
    "      Xt = X_train.iloc[train_idx]\n",
    "      Xt = Xt.drop(drop_cols, axis=1)\n",
    "      Xt = Xt.reset_index().set_index([\"prediction_id\"])\n",
    "      yt = y_train.loc[Xt.index, \"target\"]\n",
    "      wt = y_train.loc[Xt.index, \"margen\"]\n",
    "\n",
    "      Xv = X_train.iloc[valid_idx]\n",
    "      Xv = Xv.reset_index().set_index([\"prediction_id\"])\n",
    "      yv = y_train.loc[Xv.index, \"target\"]\n",
    "      wv = y_train.loc[Xv.index, \"margen\"]\n",
    "\n",
    "      learner = LGBMClassifier(n_estimators=10000, random_state=s, objective=\"binary\", metric=['GAIN'])\n",
    "      learner.fit(Xt, yt, early_stopping_rounds=10, eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=0, eval_metric=evaluar_ganancia)\n",
    "\n",
    "      test_probs.append(pd.Series(learner.predict_proba(X_test.drop(drop_cols, axis=1))[:, -1],index=X_test.index, name=\"fold_\" + str(i)))\n",
    "      train_probs2.append(pd.Series(learner.predict_proba(Xv.drop(drop_cols, axis=1))[:, -1], index=Xv.index, name=\"fold_\" + str(i)))\n",
    "\n",
    "      fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "  test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "  train_probs2 = pd.concat(train_probs2)\n",
    "  #train_probs = pd.concat(train_probs)\n",
    "\n",
    "  fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "  print(fi.sort_values(ascending=False).head(20).to_frame())\n",
    "\n",
    "  #res = y_train.join(train_probs.rename(\"probs\"))\n",
    "  #optimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen / res.margen.sum()).sum(), [(0, 1)], tol=0.00001, seed=19911125)\n",
    "  #print(\"aplicar_LGBMClassfier - \"+str(indice)+\" - Lift: \"+str(optimization['fun']))\n",
    "\n",
    "  res = y_train.join(train_probs2.rename(\"probs\"))\n",
    "  optimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen / res.margen.sum()).sum(), [(0, 1)], tol=0.00001, seed=19911125)\n",
    "  print(\"aplicar_LGBMClassfier - \"+str(indice)+\" - K-Fold Lift: \"+str(optimization['fun']))\n",
    "\n",
    "  train_probs2.to_csv(BASE_PATH+str(indice)+\"_ib_train_stacking.csv\", header=True)\n",
    "  test_probs.to_csv(BASE_PATH+str(indice)+\"_ib_test_stacking.csv\", header=True)\n",
    "\n",
    "  return optimization['fun'], fi.sort_values(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWw9BmUZExVo"
   },
   "outputs": [],
   "source": [
    "def limpiar_datasets_rcc():\n",
    "  del rcc_prod\n",
    "  del rcc_tot\n",
    "  del rcc_sdo_prod\n",
    "  del rcc_sdo_tot\n",
    "  del rcc_mora_prod\n",
    "  del rcc_mora_tot\n",
    "  del rcc_prod2\n",
    "  del rcc_bco\n",
    "  del rcc_sdo_prod2\n",
    "  del rcc_sdo_bco\n",
    "  del rcc_mora_prod2\n",
    "  del rcc_mora_bco\n",
    "\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBWGfjcXFIWH"
   },
   "source": [
    "# LGBMRegressor Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGA07559FVtw"
   },
   "outputs": [],
   "source": [
    "def aplicar_LGBMRegressor_target(indice, X_train, X_test):\n",
    "  #global X_train\n",
    "  #global X_test\n",
    "  global y_train\n",
    "  global y_test\n",
    "  global Xt\n",
    "  global yt\n",
    "  global wt\n",
    "  global Xv\n",
    "  global yv\n",
    "  global wv\n",
    "  global learner\n",
    "  global flag\n",
    "  flag = 1\n",
    "\n",
    "  def evaluar_ganancia(y_true, y_pred):\n",
    "    global flag\n",
    "    if flag == 1:\n",
    "      optimization2 = sum(y_pred * wt)\n",
    "      flag = 0\n",
    "    else:\n",
    "      optimization2 = sum(y_pred * wv)\n",
    "      flag = 1\n",
    "\n",
    "    return 'GAIN', optimization2, True\n",
    "    \n",
    "  drop_cols = [\"codmes\",\"id_persona\"]\n",
    "  fi = []\n",
    "  test_probs = []\n",
    "  train_probs = []\n",
    "  for mes in X_train.codmes.unique():\n",
    "      Xt = X_train[X_train.codmes != mes]\n",
    "      yt = y_train.loc[Xt.index, \"target\"]\n",
    "      Xt = Xt.drop(drop_cols, axis=1)    \n",
    "      wt = y_train.loc[Xt.index, \"margen\"]\n",
    "\n",
    "      Xv = X_train[X_train.codmes == mes]\n",
    "      yv = y_train.loc[Xv.index, \"target\"]\n",
    "      wv = y_train.loc[Xv.index, \"margen\"]\n",
    "\n",
    "      learner = LGBMRegressor(n_estimators=10000, random_state=19911125, metric=['GAIN'])\n",
    "      learner.fit(Xt, yt, early_stopping_rounds=10, eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=200, eval_metric=evaluar_ganancia)\n",
    "      \n",
    "      test_probs.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),index=X_test.index, name=\"fold_\" + str(mes)))\n",
    "      train_probs.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),index=Xv.index, name=\"probs\"))\n",
    "      fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "  test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "  train_probs = pd.concat(train_probs)\n",
    "  fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "  print(fi.sort_values(ascending=False).head(20).to_frame())\n",
    "\n",
    "  res = y_train.join(train_probs.rename(\"probs\"))\n",
    "  optimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen / res.margen.sum()).sum(), [(0, 1)], tol=0.00001, seed=19911125)\n",
    "  print(\"aplicar_LGBMClassfier_target - \"+str(indice)+\" - Lift: \"+str(optimization['fun']))\n",
    "\n",
    "  train_preds = (train_probs > optimization[\"x\"][0]).astype(int)\n",
    "  train_preds.index.name=\"prediction_id\"\n",
    "  train_preds.name=\"class\"\n",
    "  train_probs.to_csv(str(indice)+\"_ib_train_stacking.csv\", header=True)\n",
    "\n",
    "  test_preds = (test_probs > optimization[\"x\"][0]).astype(int)\n",
    "  test_preds.index.name=\"prediction_id\"\n",
    "  test_preds.name=\"class\"\n",
    "  test_preds.to_csv(BASE_PATH+\"benchmark1.csv\", header=True)\n",
    "  test_probs.to_csv(str(indice)+\"_ib_test_stacking.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM3TVIagMB5N"
   },
   "source": [
    "# Deep Learning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wrbw14rRMFoL"
   },
   "outputs": [],
   "source": [
    "global cn_cols\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def deep_learning_classifier(indice, k_epochs, k_folds, lr2):\n",
    "\t# Define custom loss function\n",
    "\tdef custom_loss(weight2):\n",
    "\n",
    "\t\t# KL weight (to be used by total loss and by annealing scheduler)\n",
    "\t\tweight = K.variable(weight2, name='weight')\n",
    "\t\n",
    "\t\t# Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "\t\tdef loss(y_true, y_pred):\t\t\n",
    "\t\t\treturn -sum(y_pred * weight)\n",
    "\n",
    "\t\tdef loss_auc(y_true, y_pred):\n",
    "\t\t\tauc = tf.metrics.auc(y_true, y_pred[:, -1])\n",
    "\t\t\tK.get_session().run(tf.local_variables_initializer())\n",
    "\t\t\treturn auc\n",
    "\n",
    "\t\t# Return a function\n",
    "\t\treturn loss_auc\n",
    "\n",
    "\t# define custon auc roc function\n",
    "\tdef auc_roc(y_true, y_pred):\n",
    "\t\t\t# any tensorflow metric\n",
    "\t\t\tvalue, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "\t\t\t# find all variables created for this metric\n",
    "\t\t\tmetric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "\t\t\t# Add metric variables to GLOBAL_VARIABLES collection.\n",
    "\t\t\t# They will be initialized for new session.\n",
    "\t\t\tfor v in metric_vars:\n",
    "\t\t\t\t\ttf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "\t\t\t# force to update metric values\n",
    "\t\t\twith tf.control_dependencies([update_op]):\n",
    "\t\t\t\t\tvalue = tf.identity(value)\n",
    "\t\t\t\t\treturn value\n",
    "\n",
    "\t# define wider model\n",
    "\tdef wider_model():\n",
    "\t\tglobal cn_cols\n",
    "\n",
    "\t\t# create model\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(32, input_dim=cn_cols, activation='relu'))\n",
    "\t\tmodel.add(Dropout(0.2))\n",
    "\t\tmodel.add(Dense(16, activation='relu'))\n",
    "\t\tmodel.add(Dropout(0.1))\n",
    "\t\tmodel.add(Dense(8, activation='relu'))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\t\t# Compile model\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=lr2), metrics=['accuracy',auc_roc])\n",
    "\t\t#model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9), loss=custom_loss(y_train[\"margen\"]))\n",
    "\t\n",
    "\t\treturn model\n",
    "\n",
    "\tfrom numpy.random import seed\n",
    "\tfrom tensorflow import set_random_seed\n",
    "\tseed(19911125)\n",
    "\tset_random_seed(19911125)\n",
    "\n",
    "\t# Global variables\n",
    "\tglobal Xt\n",
    "\tglobal yt\n",
    "\tglobal Xv\n",
    "\tglobal yv\n",
    "\tglobal X_train\n",
    "\tglobal y_train\n",
    "\tglobal X_test\n",
    "\tglobal y_test\n",
    "\tglobal cn_cols\n",
    "\n",
    "\tdrop_cols = [\"codmes\",\"id_persona\"]\n",
    "\tcn_cols = len(X_train.drop(drop_cols, axis=1).columns)\t\n",
    "\ti = 0\n",
    "\ttrain_probs = []\n",
    "\ttest_probs = []\n",
    "\t# Ten-Folds Cross Validation\n",
    "\tfor train_idx, valid_idx in model_selection.StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=19911125).split(X_train, y_train[\"target\"]):\n",
    "\t\t\ti += 1\n",
    "\t\t\tprint(\"Fold: \"+str(i))\n",
    "\t\t\t# Split the train & validation datasets with the cv fold ids\n",
    "\t\t\tXt = X_train.iloc[train_idx]\n",
    "\t\t\tXt = Xt.drop(drop_cols, axis=1)\n",
    "\t\t\tXt = Xt.fillna(0)\n",
    "\t\t\tres = copy.copy(y_train)\n",
    "\t\t\tres = res.loc[Xt.index]\n",
    "\t\t\tyt = y_train.loc[Xt.index, \"target\"]\n",
    "\t\t\tyt = yt.fillna(0)\n",
    "\n",
    "\t\t\tXv = X_train.iloc[valid_idx]\n",
    "\t\t\tXv = Xv.drop(drop_cols, axis=1)\n",
    "\t\t\tXv = Xv.fillna(0)\n",
    "\t\t\tyv = y_train.loc[Xv.index, \"target\"]\n",
    "\t\t\tyv = yv.fillna(0)\n",
    "\t\t\t \n",
    "\t\t\t# evaluate model with standardized dataset\n",
    "\t\t\tscaler = StandardScaler()\n",
    "\t\t\tscaler = scaler.fit(Xt)\n",
    "\t\t\tXt2 = scaler.transform(Xt)\n",
    "\t\t\tXv2 = scaler.transform(Xv)\n",
    "\t\t\tX_test2 = scaler.transform(X_test.drop(drop_cols, axis=1))\n",
    "\n",
    "\t\t\tkr = KerasClassifier(build_fn=wider_model, batch_size=128, verbose=0)\n",
    "\t\t\tkr2 = kr.fit(Xt2, yt, epochs=k_epochs, verbose=0, validation_data=(Xv2, yv))\n",
    "\n",
    "\t\t\ttt_probs = kr.predict_proba(Xt2)[:,-1]\n",
    "\t\t\tres[\"probs\"] = tt_probs\n",
    "\t\t\toptimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen / res.margen.sum()).sum(), [(0, 1)], tol=0.0001, seed=19911125)\n",
    "\t\t\tprint(\"Lift: \"+str(optimization['fun']))\n",
    "\t\t\tdel res\n",
    "\n",
    "\t\t\ttrain_probs.append(pd.Series(kr.predict_proba(Xv2)[:,-1],index=Xv.index, name=\"fold_\"+str(i)))\n",
    "\t\t\ttest_probs.append(pd.Series(kr.predict_proba(X_test2)[:,-1],index=X_test.index, name=\"fold_\"+str(i)))\n",
    "\n",
    "\t# Agrupo las predicciones, en el caso de validation por separado en el caso de test por la media\n",
    "\ttest_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "\ttrain_probs = pd.concat(train_probs)\n",
    "\n",
    "\t# Calculo el lift en ganancia para validation\n",
    "\tres = copy.copy(y_train)\n",
    "\tres[\"probs\"] = train_probs\n",
    "\toptimization = differential_evolution(lambda c: -((res.probs > c[0]) * res.margen / res.margen.sum()).sum(), [(0, 1)], tol=0.0001, seed=19911125)\n",
    "\tprint(\"Lift: \"+str(optimization['fun']))\n",
    "\n",
    "\ttest_probs.to_csv(str(indice)+\"_ib_test_stacking.csv\", header=True)\n",
    "\ttrain_probs.to_csv(str(indice)+\"_ib_train_stacking.csv\", header=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayeegaIndZWz"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-HIfwjbdXai"
   },
   "outputs": [],
   "source": [
    "def stacking(indice, BINING, nbins):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  # Leo el resultado del algoritmo de clasificacion\n",
    "  X_train_st = pd.read_csv(BASE_PATH+str(indice)+\"_ib_train_stacking.csv\")\n",
    "  X_train_st = X_train_st.reset_index()\n",
    "  try:\n",
    "    X_train_st = X_train_st.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    w=0\n",
    "  try:\n",
    "    X_train_st = X_train_st.drop([\"Unnamed: 0\"], axis=1)\n",
    "  except:\n",
    "    w=0\n",
    "  X_train_st.columns = [\"prediction_id\", indice+\"_prob\"]\n",
    "  X_train_st = X_train_st.fillna(0)\n",
    "\n",
    "  X_test_st = pd.read_csv(BASE_PATH+str(indice)+\"_ib_test_stacking.csv\")\n",
    "  X_test_st = X_test_st.reset_index()\n",
    "  try:\n",
    "    X_test_st = X_test_st.drop([\"Unnamed: 0\"], axis=1)\n",
    "  except:\n",
    "    w=0\n",
    "  try:\n",
    "    X_test_st = X_test_st.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    w=0\n",
    "  X_test_st.columns = [\"prediction_id\", indice+\"_prob\"]\n",
    "  X_test_st = X_test_st.fillna(0)\n",
    "\n",
    "  if BINING:\n",
    "    # Realizo el bining tanto en train como en test\n",
    "    cff.binning_feature(X_train_st, [indice+\"_prob\"], 1, KBinsDiscretizer(n_bins=nbins, encode='ordinal', strategy='uniform'))\n",
    "    cff.binning_feature(X_test_st, [indice+\"_prob\"], 1, KBinsDiscretizer(n_bins=nbins, encode='ordinal', strategy='uniform'))\n",
    "\n",
    "  # Agrego los datos al dataset de training (stacking)\n",
    "  X_train = X_train.merge(X_train_st, how=\"left\", on=[\"prediction_id\"])\n",
    "  X_test = X_test.merge(X_test_st, how=\"left\", on=[\"prediction_id\"])\n",
    "  \n",
    "  if BINING:\n",
    "    # Borro la variable original\n",
    "    X_train = X_train.drop([indice+\"_prob\"], axis=1)\n",
    "    X_test = X_test.drop([indice+\"_prob\"], axis=1)\n",
    "\n",
    "  X_test = X_test.reset_index().set_index([\"prediction_id\"])\n",
    "  X_train = X_train.reset_index().set_index([\"prediction_id\"])\n",
    "\n",
    "  try:\n",
    "    X_train = X_train.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    print(\"Column doesn't exists\")\n",
    "    \n",
    "  try:\n",
    "    X_test = X_test.drop([\"index\"], axis=1)\n",
    "  except:\n",
    "    print(\"Column doesn't exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXGcdEHFVBJ-"
   },
   "source": [
    "# LGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0mV8NYAVAFc"
   },
   "outputs": [],
   "source": [
    "def aplicar_LGBMRegressor(indice, kseeds, ENTRENAR_TODOS_MESES, k_folds, params):\n",
    "  global X_train\n",
    "  global y_train\n",
    "  global X_test\n",
    "  global y_test\n",
    "  global Xt\n",
    "  global yt\n",
    "  global wt\n",
    "  global Xv\n",
    "  global yv\n",
    "  global wv\n",
    "  global learner\n",
    "  global flag\n",
    "  flag = 1\n",
    "\n",
    "  def evaluar_ganancia(y_true, y_pred):\n",
    "    global flag\n",
    "    #print(y_pred)\n",
    "    if flag == 1:\n",
    "      optimization2 = (sum([ 1 if v > 0 else 0 for v in y_pred ] * wt) / wt.sum()).sum()\n",
    "      flag = 0\n",
    "    else:\n",
    "      optimization2 = (sum([ 1 if v > 0 else 0 for v in y_pred ] * wv) / wv.sum()).sum()\n",
    "      flag = 1\n",
    "\n",
    "    return 'GAIN', optimization2, True\n",
    "\n",
    "  drop_cols = [\"codmes\",\"id_persona\"]\n",
    "  fi = []\n",
    "  test_probs = []\n",
    "  train_probs = []\n",
    "  for s in kseeds:\n",
    "    print(\"Seed: \"+str(s))\n",
    "    #for mes in X_train.codmes.unique():\n",
    "        #Xt = X_train[X_train.codmes != mes]\n",
    "        #yt = y_train.loc[Xt.index, \"margen\"]\n",
    "        #Xt = Xt.drop(drop_cols, axis=1)    \n",
    "        #wt = y_train.loc[Xt.index, \"margen\"]\n",
    "\n",
    "        #Xv = X_train[X_train.codmes == mes]\n",
    "        #yv = y_train.loc[Xv.index, \"margen\"]\n",
    "        #wv = y_train.loc[Xv.index, \"margen\"]\n",
    "\n",
    "        #if params != None:\n",
    "          #learner = LGBMRegressor(n_estimators=10000, random_state=s, \n",
    "                                  #metric=['GAIN'],  \n",
    "                                  #max_depth=params[\"max_depth\"], \n",
    "                                  #gamma=params[\"gamma\"], \n",
    "                                  #colsample_bytree=params[\"colsample_bytree\"])\n",
    "        #else:\n",
    "          #learner = LGBMRegressor(n_estimators=10000, random_state=s, metric=['GAIN'])\n",
    "\n",
    "        #learner.fit(Xt, yt, early_stopping_rounds=30, eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=0, eval_metric=evaluar_ganancia)\n",
    "\n",
    "        #if not(ENTRENAR_TODOS_MESES):\n",
    "          #test_probs.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),index=X_test.index, name=\"fold_\" + str(mes)))\n",
    "\n",
    "        #train_probs.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),index=Xv.index, name=\"probs\"))\n",
    "        #fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "    if ENTRENAR_TODOS_MESES:  \n",
    "      i = 0\n",
    "      for train_idx, valid_idx in model_selection.StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=s).split(X_train, y_train[\"target\"]):\n",
    "          i += 1\n",
    "          \n",
    "          # Split the train & validation datasets with the cv fold ids\n",
    "          Xt = X_train.iloc[train_idx]\n",
    "          Xt = Xt.drop(drop_cols, axis=1)\n",
    "          yt = y_train.loc[Xt.index, \"margen\"]\n",
    "          wt = y_train.loc[Xt.index, \"margen\"]\n",
    "\n",
    "          Xv = X_train.iloc[valid_idx]\n",
    "          yv = y_train.loc[Xv.index, \"margen\"]\n",
    "          wv = y_train.loc[Xv.index, \"margen\"]\n",
    "\n",
    "          learner = LGBMRegressor(n_estimators=10000, random_state=s, metric=['GAIN'])\n",
    "          learner.fit(Xt, yt, early_stopping_rounds=20, eval_set=[(Xt, yt), (Xv.drop(drop_cols, axis=1), yv)], verbose=0, eval_metric=evaluar_ganancia)\n",
    "\n",
    "          train_probs.append(pd.Series(learner.predict(Xv.drop(drop_cols, axis=1)),index=Xv.index, name=\"fold_tot\"+str(i)))\n",
    "          test_probs.append(pd.Series(learner.predict(X_test.drop(drop_cols, axis=1)),index=X_test.index, name=\"fold_tot\"+str(i)))\n",
    "\n",
    "          fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "  # Calculo la salida final\n",
    "  #train_probs = pd.concat(train_probs)\n",
    "  train_probs = pd.concat(train_probs, axis=1).mean(axis=1)\n",
    "  test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "  fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "  res = y_train.join(train_probs.rename(\"probs\"))\n",
    "  optimization = differential_evolution(lambda c: -(sum([1 if v > c[0] else 0 for v in res.probs] * res.margen) / res.margen.sum()).sum(), [(-1, 1)], tol=0.0001, seed=19911125)\n",
    "  \n",
    "  if params == None:\n",
    "    print(\"Lift: \"+str(optimization['fun']))\n",
    "\n",
    "  lift = -(sum([1 if v > 0 else 0 for v in res.probs] * res.margen) / res.margen.sum()).sum()\n",
    "  if params == None:\n",
    "    print(\"Lift: \"+str(lift))\n",
    "\n",
    "  if params == None:\n",
    "    print(fi.sort_values(ascending=False).head(20).to_frame())\n",
    "\n",
    "  #test_preds = (test_probs > optimization['x'][0]).astype(int)\n",
    "  test_preds = (test_probs > 0).astype(int)\n",
    "  test_preds.index.name=\"prediction_id\"\n",
    "  test_preds.name=\"class\"\n",
    "  test_preds.to_csv(BASE_PATH+indice+\"_benchmark_regressor_stacking.csv\", header=True)\n",
    "  \n",
    "  test_probs.index.name=\"prediction_id\"\n",
    "  test_preds.name=\"prob\"\n",
    "  test_probs.to_csv(BASE_PATH+indice+\"_ib_test_stacking.csv\", header=True)\n",
    "  \n",
    "  train_probs.index.name=\"prediction_id\"\n",
    "  train_probs.name=\"prob\"\n",
    "  train_probs.to_csv(BASE_PATH+indice+\"_ib_train_stacking.csv\", header=True)\n",
    "\n",
    "  return -1 * (optimization['fun']), fi.sort_values(ascending=False).to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4oetS6hGwl_"
   },
   "source": [
    "# Pruebo combinaciones de modelos, stacking y variables agregadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhCHSwJrG27_"
   },
   "outputs": [],
   "source": [
    "# Guardo un bkp de los datasets base\n",
    "X_train_base = copy.copy(X_train)\n",
    "X_test_base = copy.copy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iXlLQ1jKDzB"
   },
   "outputs": [],
   "source": [
    "X_train = copy.copy(X_train_base)\n",
    "X_test = copy.copy(X_test_base)\n",
    "\n",
    "# Agrego las variables historicas de RCC de 10 meses\n",
    "agregar_rcc_10m(True)\n",
    "#agregar_rcc_10m(False)\n",
    "\n",
    "# Construyo un modelo con estas variables\n",
    "#aplicar_LGBMClassfier(\"10m\")\n",
    "\n",
    "# Realizo el stacking de este model al dataset base\n",
    "#stacking(\"10m\", True, 100)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qhp4p8OwQ4Hd"
   },
   "outputs": [],
   "source": [
    "# Agrego las variables historicas de RCC de 10 meses\n",
    "agregar_rcc_3m(True)\n",
    "#agregar_rcc_3m(False)\n",
    "\n",
    "# Construyo un modelo con estas variables\n",
    "#aplicar_LGBMClassfier(\"3m\")\n",
    "\n",
    "# Realizo el stacking de este model al dataset base\n",
    "#stacking(\"3m\", True, 100)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIYaXWOygbWX"
   },
   "outputs": [],
   "source": [
    "# Guardo un bkp de los datasets base\n",
    "X_train_3m_10m = copy.copy(X_train)\n",
    "X_test_3m_10m = copy.copy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw1EOuNISQWa"
   },
   "outputs": [],
   "source": [
    "# Guardo un bkp de los datasets base\n",
    "X_train = copy.copy(X_train_3m_10m)\n",
    "X_test = copy.copy(X_test_3m_10m)\n",
    "\n",
    "# Agrego las variables historicas de RCC de 10 meses\n",
    "comparar_linea_otros_bancos(10)\n",
    "\n",
    "# Construyo un modelo con estas variables\n",
    "aplicar_LGBMClassfier(\"om\", [20191125, 19770804, 19870605, 19721010, 19761126], True, 7)\n",
    "\n",
    "# Realizo el stacking de este model al dataset base, antes guardo el dataset original\n",
    "stacking(\"om\", True, 1000)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oXDytIIGzJ-"
   },
   "outputs": [],
   "source": [
    "# Guardo un bkp de los datasets base\n",
    "X_train_om = copy.copy(X_train)\n",
    "X_test_om = copy.copy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvtaVVHcN0rx"
   },
   "outputs": [],
   "source": [
    "#X_train = copy.copy(X_train_om)\n",
    "#X_test = copy.copy(X_test_om)\n",
    "#var_tend_hist()\n",
    "#X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WohspHed6Ry"
   },
   "outputs": [],
   "source": [
    "gain, fi = aplicar_LGBMRegressor(\"lgbmreg_base\", [20191125], True, 7, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgg9jXf_1QOB"
   },
   "outputs": [],
   "source": [
    "# Guardo un bkp de los datasets base\n",
    "X_train_fe2 = copy.copy(X_train)\n",
    "X_test_fe2 = copy.copy(X_test)\n",
    "fi = fi.reset_index()\n",
    "fi.columns = [\"var\", \"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIl4mgRItIdt"
   },
   "outputs": [],
   "source": [
    "X_train = copy.copy(X_train_fe2)\n",
    "X_test = copy.copy(X_test_fe2)\n",
    "\n",
    "dt = fi[fi[\"importance\"] > 0.005]\n",
    "dt = dt.append(pd.Series({\"var\":\"id_persona\", \"importance\":1}), ignore_index=True)\n",
    "dt = dt.append(pd.Series({\"var\":\"codmes\", \"importance\":1}), ignore_index=True)\n",
    "\n",
    "print(dt.shape)\n",
    "print(fi.shape)\n",
    "\n",
    "X_train = X_train[dt[\"var\"]]\n",
    "X_test = X_test[dt[\"var\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKM7AxsG9ozJ"
   },
   "outputs": [],
   "source": [
    "gain_fe, fi_fe = aplicar_LGBMRegressor(\"lgbmreg_base_fe\", [20191125], True, 7, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DV3YT7s8vxUQ"
   },
   "source": [
    "# Tuning Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGsDwMytvzjc"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "def objective(params):\n",
    "  params2 = {\n",
    "      'max_depth': int(params['max_depth']),\n",
    "      'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "      'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "  }\n",
    "\n",
    "  gain = aplicar_LGBMRegressor(\"lgbmreg\", False, 10, params2)\n",
    "\n",
    "  print(\"Gain: \"+str(gain)+\" Params:{\"+str(params2)+\"}\")\n",
    "\n",
    "  return (-1 * gain)\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 6, 40, 4),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.9),\n",
    "    'gamma': hp.uniform('gamma', 0.2, 0.4)\n",
    "}\n",
    "\n",
    "#best = fmin(fn=objective,space=space,algo=tpe.suggest,max_evals=300)\n",
    "#print(\"Best: \"+str(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Cyp1NQaKVxx"
   },
   "outputs": [],
   "source": [
    "# Best\n",
    "#params2 = {\n",
    "    #'max_depth': int(best['max_depth']),\n",
    "    #'gamma': \"{:.3f}\".format(best['gamma']),\n",
    "    #'colsample_bytree': '{:.3f}'.format(best['colsample_bytree']),\n",
    "#}\n",
    "#gain = aplicar_LGBMRegressor(\"lgbmreg_best\", True, 10, params2)\n",
    "#print(\"Gain: \"+str(gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROGLTdSYJZ7O"
   },
   "outputs": [],
   "source": [
    "#Gain: 1.1823120941202543 Params:{{'max_depth': 7, 'gamma': '0.277', 'colsample_bytree': '0.574'}}\n",
    "#gain = aplicar_LGBMRegressor(\"lgbmreg_2do\", True, 10, {'max_depth': 7, 'gamma': '0.277', 'colsample_bytree': '0.574'})\n",
    "#print(\"Gain: \"+str(gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60GK43eqURmv"
   },
   "outputs": [],
   "source": [
    "#Gain: 1.1807729124945683 Params:{{'max_depth': 6, 'gamma': '0.230', 'colsample_bytree': '0.570'}}\n",
    "#gain = aplicar_LGBMRegressor(\"lgbmreg_1er\", True, 10, {'max_depth': 6, 'gamma': '0.230', 'colsample_bytree': '0.570'})\n",
    "#print(\"Gain: \"+str(gain))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "benchmark-interbank-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
